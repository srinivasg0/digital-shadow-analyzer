{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NVJUmU-SiPUK",
        "outputId": "4f39d070-f4af-4d62-bef2-c56ac60ade70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Trying to load en_core_web_trf...\n",
            "‚ö†Ô∏è en_core_web_trf not found. Installing fallback en_core_web_sm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Exposure Score: 100 /100 ===\n",
            "\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.8548497557640076}]\n",
            "\n",
            "Evidence found:\n",
            "- [regex] srinivas.reddy@example.com (email) ‚Üí +25.00\n",
            "- [regex] https://srinivasportfolio.com (url) ‚Üí +10.00\n",
            "- [spacy] Srinivas (PERSON) ‚Üí +7.00\n",
            "- [spacy] Infosys Technologies (ORG) ‚Üí +10.40\n",
            "- [spacy] Bangalore (PERSON) ‚Üí +7.50\n",
            "- [spacy] 98765 12345 (DATE) ‚Üí +4.53\n",
            "- [spacy] 8:00 AM (TIME) ‚Üí +2.60\n",
            "- [spacy] New York (GPE) ‚Üí +7.00\n",
            "- [spacy] 12th August 2023 (DATE) ‚Üí +5.87\n",
            "- [hf] Srini (PER) ‚Üí +1.47\n",
            "- [hf] Infosys Technologies (ORG) ‚Üí +10.39\n",
            "- [hf] BangaloreNew York (LOC) ‚Üí +9.19\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c84a49be25fb927c81.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c84a49be25fb927c81.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# =============================\n",
        "# 1) Setup imports\n",
        "# =============================\n",
        "import re, json, subprocess, sys\n",
        "import spacy\n",
        "from typing import List, Dict, Any\n",
        "from transformers import pipeline\n",
        "\n",
        "# =============================\n",
        "# 2) spaCy loader with fallback\n",
        "# =============================\n",
        "def load_spacy_model():\n",
        "    try:\n",
        "        print(\"üîç Trying to load en_core_web_trf...\")\n",
        "        return spacy.load(\"en_core_web_trf\")\n",
        "    except OSError:\n",
        "        print(\"‚ö†Ô∏è en_core_web_trf not found. Installing fallback en_core_web_sm...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "        return spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = load_spacy_model()\n",
        "\n",
        "# =============================\n",
        "# 3) Hugging Face pipelines\n",
        "# =============================\n",
        "hf_ner = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"dslim/bert-base-NER\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "sentiment = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "\n",
        "# =============================\n",
        "# 4) Regex PII patterns\n",
        "# =============================\n",
        "PII_PATTERNS = {\n",
        "    \"email\": re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'),\n",
        "    \"phone\": re.compile(r'\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{6,10}\\b'),\n",
        "    \"address_like\": re.compile(r'\\b\\d{1,5}\\s+(?:[A-Za-z0-9]+\\s){0,4}(?:street|st|road|rd|avenue|ave|lane|ln|boulevard|blvd|drive|dr)\\b', re.I),\n",
        "    \"url\": re.compile(r'\\bhttps?://[^\\s]+\\b'),\n",
        "    \"credit_card_like\": re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b'),\n",
        "}\n",
        "\n",
        "def detect_pii_regex(text: str) -> List[Dict[str,str]]:\n",
        "    hits = []\n",
        "    for label, patt in PII_PATTERNS.items():\n",
        "        for m in patt.finditer(text):\n",
        "            hits.append({\"type\": label, \"text\": m.group(0), \"span\": [m.start(), m.end()]})\n",
        "    return hits\n",
        "\n",
        "# =============================\n",
        "# 5) Entity extractors\n",
        "# =============================\n",
        "def run_spacy_ner(text: str):\n",
        "    doc = nlp(text)\n",
        "    return [{\"source\":\"spacy\", \"label\": ent.label_, \"text\": ent.text, \"span\":[ent.start_char, ent.end_char]} for ent in doc.ents]\n",
        "\n",
        "def run_hf_ner(text: str):\n",
        "    out = hf_ner(text)\n",
        "    merged = []\n",
        "    current = None\n",
        "\n",
        "    # Merge sub-tokens (e.g., In + ##fosys ‚Üí Infosys)\n",
        "    for e in out:\n",
        "        label = e[\"entity_group\"]\n",
        "        word = e[\"word\"].replace(\"##\", \"\")\n",
        "        if current and current[\"label\"] == label:\n",
        "            current[\"text\"] += word\n",
        "            current[\"score\"] = max(current[\"score\"], e[\"score\"])\n",
        "        else:\n",
        "            if current:\n",
        "                merged.append(current)\n",
        "            current = {\"source\":\"hf\", \"label\": label, \"text\": word, \"score\": e[\"score\"]}\n",
        "    if current:\n",
        "        merged.append(current)\n",
        "\n",
        "    return merged\n",
        "\n",
        "def extract_evidence(text: str):\n",
        "    evidence = []\n",
        "    evidence.extend(detect_pii_regex(text))\n",
        "    evidence.extend(run_spacy_ner(text))\n",
        "    evidence.extend(run_hf_ner(text))\n",
        "    return evidence\n",
        "\n",
        "# =============================\n",
        "# 6) Scoring system\n",
        "# =============================\n",
        "WEIGHTS = {\n",
        "    \"email\": 25, \"phone\": 25, \"credit_card_like\": 40, \"url\": 10, \"address_like\": 20,\n",
        "    \"PERSON\": 15, \"GPE\": 15, \"LOC\": 12, \"ORG\": 12, \"DATE\": 8, \"TIME\": 6, \"MONEY\": 20,\n",
        "    \"default_entity\": 5,\n",
        "}\n",
        "\n",
        "def compute_exposure_score(evidence):\n",
        "    total, details = 0, []\n",
        "    for e in evidence:\n",
        "        t = e.get(\"type\") or e.get(\"label\") or \"\"\n",
        "        w = WEIGHTS.get(t, WEIGHTS.get(t.upper(), WEIGHTS[\"default_entity\"]))\n",
        "        score_factor = float(e.get(\"score\", 1.0))\n",
        "        text_len_factor = min(1.0, len(e.get(\"text\",\"\")) / 30.0 + 0.2)\n",
        "        add = w * score_factor * text_len_factor\n",
        "        details.append({\"evidence\": e, \"weight\": w, \"factor\": score_factor * text_len_factor, \"contribution\": add})\n",
        "        total += add\n",
        "    return {\"raw_score\": total, \"exposure_score\": min(100, round(total, 2)), \"details\": details}\n",
        "\n",
        "# =============================\n",
        "# 7) Analyzer\n",
        "# =============================\n",
        "def analyze_text(text: str):\n",
        "    if not text.strip():\n",
        "        return {\"error\":\"empty text\"}\n",
        "    evidence = extract_evidence(text)\n",
        "    score = compute_exposure_score(evidence)\n",
        "    return {\"input_text\": text, \"sentiment\": sentiment(text[:512]), \"evidence\": evidence, \"score\": score}\n",
        "\n",
        "# =============================\n",
        "# 8) Pretty printing\n",
        "# =============================\n",
        "def pretty_print_result(res):\n",
        "    if \"error\" in res: print(res[\"error\"]); return\n",
        "    print(\"=== Exposure Score:\", res[\"score\"][\"exposure_score\"], \"/100 ===\\n\")\n",
        "    print(\"Sentiment:\", res[\"sentiment\"])\n",
        "    print(\"\\nEvidence found:\")\n",
        "    for d in res[\"score\"][\"details\"]:\n",
        "        e = d[\"evidence\"]\n",
        "        print(f\"- [{e.get('source','regex')}] {e.get('text')} ({e.get('type') or e.get('label')}) ‚Üí +{d['contribution']:.2f}\")\n",
        "\n",
        "# =============================\n",
        "# 9) Test run\n",
        "# =============================\n",
        "sample = \"\"\"\n",
        "My name is Srinivas, and I have been working at Infosys Technologies in Bangalore.\n",
        "You can reach me at srinivas.reddy@example.com or call me at +91 98765 12345.\n",
        "Every morning, I leave my apartment around 8:00 AM to go to the office.\n",
        "I traveled to New York on 12th August 2023 for a meeting.\n",
        "Check my site: https://srinivasportfolio.com\n",
        "\"\"\"\n",
        "res = analyze_text(sample)\n",
        "pretty_print_result(res)\n",
        "\n",
        "# =============================\n",
        "# 10) (Optional) Gradio UI\n",
        "# =============================\n",
        "import gradio as gr\n",
        "\n",
        "def gradio_analyze(text):\n",
        "    res = analyze_text(text)\n",
        "    if \"error\" in res:\n",
        "        return \"<b>No text provided.</b>\"\n",
        "    html = f\"<h3>Exposure Score: {res['score']['exposure_score']}/100</h3>\"\n",
        "    html += \"<h4>Evidence</h4><ul>\"\n",
        "    for d in res[\"score\"][\"details\"]:\n",
        "        e = d[\"evidence\"]\n",
        "        label = e.get('type') or e.get('label') or 'unknown'\n",
        "        html += f\"<li><b>{label}</b>: {e.get('text')}</li>\"\n",
        "    html += \"</ul>\"\n",
        "    html += f\"<h4>Sentiment</h4><pre>{res['sentiment']}</pre>\"\n",
        "    return html\n",
        "\n",
        "# ‚úÖ Launch Gradio properly\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_analyze,\n",
        "    inputs=gr.Textbox(lines=6, placeholder=\"Enter your text here...\"),\n",
        "    outputs=\"html\",\n",
        "    title=\"Digital Shadow ‚Äî Text Analyzer\"\n",
        ")\n",
        "\n",
        "# For Colab use share=True, for local just demo.launch()\n",
        "demo.launch(share=True, debug=True)\n",
        "\n"
      ]
    }
  ]
}