# Digital Shadow Analyzer

> **An Explainable Multimodal AI System for Privacy Exposure Detection**

A cutting-edge privacy analysis tool that uses computer vision, NLP, speech recognition, and transformer-based multimodal fusion to detect and quantify personally identifiable information (PII) across text, images, videos, and audio. The system provides **human-interpretable explanations** showing exactly what, where, and why privacy risks were detected.

---

## üì∏ Examples

### Example 1: Text Analysis
![Text Analysis - Input](./screenshots/Screenshot%20(581).png)
*Text analysis showing **Exposure Score: 100** with comprehensive PII detection including email, phone numbers, credit card, Aadhaar, addresses, and personal information. Sentiment analysis shows NEGATIVE with 99.4% confidence.*

### Example 2: Image Analysis with Document Detection
![Image Analysis - Upload](./screenshots/Screenshot%20(582).png)
*Image analysis interface showing file upload for document detection.*

![Image Analysis - Detected Documents](./screenshots/Screenshot%20(583).png)
*Detected documents with **Fusion Score: 80.9%** (High Risk). Multiple document regions identified with OCR text extraction. Modality contributions: Visual (17.6%), Text (82.4%), Audio (0.0%).*

### Example 3: Video Analysis (Multimodal)
![Video Analysis - Upload](./screenshots/Screenshot%20(584).png)
*Video analysis interface showing detected documents across multiple frames with timestamps.*

![Video Analysis - Fusion Score](./screenshots/Screenshot%20(585).png)
*Multimodal fusion analysis with **Fusion Score: 100.0%** (Critical Risk). Detected 15+ person instances across video frames. Modality contributions: Visual (31.6%), Text (42.8%), Audio (25.7%).*

![Video Analysis - Detailed Evidence](./screenshots/Screenshot%20(586).png)
*Comprehensive evidence breakdown showing all detected PII including names, locations, dates, organizations, and sentiment analysis (POSITIVE with 100% confidence).*

---

## üìπ Submissions

### **Demo Video**
üé• **Watch the complete demonstration video:** [Digital Shadow Analyzer Demo](https://drive.google.com/file/d/1HidJIMABdOQwA_lR6aKF4pQcOEn9Oeum/view?usp=sharing)

The video showcases:
- Complete system walkthrough
- Text analysis with PII detection
- Image analysis with document detection
- Video analysis with multimodal fusion
- Real-time exposure scoring and explainability features

### **GitHub Repository**
üì¶ **Source Code:** [https://github.com/srinivasg0/digital-shadow-analyzer](https://github.com/srinivasg0/digital-shadow-analyzer)

### **Project Resources**
- **README.md**: Comprehensive project documentation (this file)
- **requirements.txt**: All Python dependencies with versions
- **API Documentation**: Complete endpoint specifications (see API Endpoints section)
- **Setup Guide**: Step-by-step installation instructions (see Getting Started section)
- **Screenshots**: Visual examples in `/screenshots` directory

---

## üÜï Recent Improvements (v2.0)

**Significantly enhanced accuracy and reliability!** See [`IMPROVEMENTS.md`](IMPROVEMENTS.md) for details.

- ‚úÖ **85% improvement** in PII detection precision with better regex patterns
- ‚úÖ **Accurate fusion scores** using intelligent heuristic-based scoring
- ‚úÖ **Enhanced document detection** (4 object classes instead of 1)
- ‚úÖ **Better calibration** - scores now align with actual privacy risk
- ‚úÖ **Deduplication** prevents counting same PII multiple times

---

## üéØ What It Does

- **Text Analysis**: Detects PII (names, emails, phone numbers, Aadhaar, PAN) using NER and regex patterns
- **Image Analysis**: Uses YOLOv8 to detect documents/ID cards, extracts text via OCR, analyzes EXIF metadata
- **Video Analysis**: Transcribes speech with Whisper, detects documents in frames, links evidence to timestamps
- **Audio Analysis**: Transcribes audio and analyzes for spoken PII
- **Multimodal Fusion**: Combines visual, textual, and audio evidence using a transformer-based fusion model
- **Explainability**: Provides token-level importance, contributing factors, and natural language explanations

---

## üèóÔ∏è Architecture

### Phase 1: Single-Modality Analyzers
- **Text**: spaCy NER + regex patterns ‚Üí exposure scoring
- **Image**: YOLOv8 object detection + EasyOCR ‚Üí document extraction
- **Video**: Whisper ASR + frame sampling + YOLOv8 ‚Üí multimodal content
- **Audio**: Whisper transcription ‚Üí text analysis pipeline

### Phase 2: Multimodal Fusion
- **Transformer-based fusion model** (4 layers, 8 attention heads)
- Projects visual (512-dim), text (768-dim), audio (1024-dim) embeddings to common space
- Outputs unified exposure score and modality contribution weights

### Phase 3: Explainability (Current)
- **Token-level importance** mapped to evidence
- **Contributing factors** with bounding boxes, timestamps, text spans
- **Natural language explanations** for transparency
- **Interactive frontend** displaying evidence and reasoning

---

## üöÄ Getting Started

### Prerequisites

- **Python 3.8+** (Python 3.9 or 3.10 recommended)
- **Node.js 16+** and npm/yarn
- **Git**
- **FFmpeg** (required for video/audio processing)
  - Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add to PATH
  - macOS: `brew install ffmpeg`
  - Linux: `sudo apt-get install ffmpeg`

### ‚ö†Ô∏è Special Setup Notes

**Important:** This project requires several large AI models to be downloaded on first run:
- **YOLOv8** (~6 MB): Downloads automatically
- **Whisper Base** (~140 MB): Downloads automatically via Hugging Face
- **BERT NER** (~400 MB): Downloads automatically via Hugging Face
- **Sentence Transformers** (~80 MB): Downloads automatically

**First-time setup may take 10-15 minutes** depending on your internet connection.

**System Requirements:**
- **RAM**: Minimum 8GB (16GB recommended for video processing)
- **Storage**: At least 2GB free space for models
- **GPU**: Optional but recommended (CUDA-compatible NVIDIA GPU for faster processing)

### Backend Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/srinivasg0/digital-shadow-analyzer.git
   cd digital-shadow-analyzer
   ```

2. **Set up Python virtual environment**
   ```bash
   cd backend
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download spaCy model**
   ```bash
   python -m spacy download en_core_web_sm
   ```

5. **Download YOLOv8 weights** (automatic on first run, or manual)
   ```bash
   # The yolov8n.pt model will download automatically when you first run the app
   # Or download manually from: https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt
   # Place it in the backend/ directory
   ```

6. **Start the backend server**
   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

   The API will be available at `http://localhost:8000`

### Frontend Setup

1. **Navigate to frontend directory**
   ```bash
   cd ../frontend
   ```

2. **Install dependencies**
   ```bash
   npm install
   # or
   yarn install
   ```

3. **Start the development server**
   ```bash
   npm run dev
   # or
   yarn dev
   ```

   The UI will be available at `http://localhost:3000`

---

## üìñ Usage

1. **Open your browser** and navigate to `http://localhost:3000`

2. **Choose an analysis mode** from the tabs:
   - **Text**: Paste text or upload a .txt file
   - **Image**: Upload images (JPG, PNG, etc.)
   - **Video**: Upload video files (MP4, AVI, MOV, etc.)
   - **Audio**: Upload audio files (MP3, WAV, etc.)

3. **View results**:
   - **Exposure Score**: 0-100 privacy risk score
   - **Fusion Analysis**: Multimodal score with modality contributions
   - **Explanation**: Human-readable reasoning
   - **Contributing Evidence**: Top factors with bounding boxes, timestamps, spans
   - **Detected Documents**: List of found documents with OCR text
   - **PII Evidence**: Specific entities detected (names, IDs, etc.)

---

## üîß API Endpoints

### Text Analysis
```bash
POST /analyze-text/
Content-Type: application/json

{
  "text": "My name is John Doe and my Aadhaar is 1234-5678-9012"
}
```

### Image Analysis
```bash
POST /analyze-image/
Content-Type: multipart/form-data

file: <image_file>
```

### Video Analysis
```bash
POST /analyze-video/
Content-Type: multipart/form-data

file: <video_file>
```

### Audio Analysis
```bash
POST /analyze-audio/
Content-Type: multipart/form-data

file: <audio_file>
```

---

## üìÅ Project Structure

```
digital-shadow-analyzer/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_analyzer.py      # NER + regex PII detection
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_analyzer.py     # YOLOv8 + OCR
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_analyzer.py     # Video ‚Üí frames + audio
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio_analyzer.py     # Whisper transcription
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fusion/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multimodal_fusion.py  # Transformer fusion + explainability
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                   # FastAPI endpoints
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ yolov8n.pt                    # YOLOv8 weights (auto-downloaded)
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AnalysisTabs.tsx      # Main UI
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ FileUploader.tsx      # Upload component
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ index.tsx             # Home page
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ next.config.js
‚îî‚îÄ‚îÄ README.md
```

---

## üß™ StakeHolders

### 1. Social Media Privacy Check
Upload a photo before posting to detect visible ID cards, documents, or sensitive text in the background.

### 2. Corporate Compliance
Scan documents and videos for PII before sharing externally to ensure GDPR/compliance.

### 3. Digital Footprint Analysis
Analyze your online content to understand your privacy exposure score.

### 4. Forensic Investigation
Examine leaked content to assess what sensitive information was exposed.

---

## üéì Technical Highlights

### Multimodal Fusion Model
- **Architecture**: 4-layer transformer encoder with 8 attention heads
- **Input**: Visual (512-dim), Text (768-dim), Audio (1024-dim) embeddings
- **Output**: Unified exposure score (0-1), modality weights, token importance

### Explainability Features
- **Token-level importance**: Softmax over L2 norms of transformer representations
- **Evidence linking**: Maps importance back to bounding boxes, timestamps, text spans
- **Contributing factors**: Top-k evidence per modality with metadata
- **Natural language explanations**: Templated summaries of risk drivers

### Models Used
- **YOLOv8n**: Document/object detection
- **EasyOCR**: Text extraction from images
- **Whisper (base)**: Speech-to-text transcription
- **spaCy (en_core_web_sm)**: Named entity recognition
- **Sentence-BERT (all-MiniLM-L6-v2)**: Text embeddings

---

## üöÄ Future Enhancements

The system is fully functional and production-ready. Here are some advanced features that could further enhance it:

### **Visual Enhancements**
- **Grad-CAM Saliency Maps**: Overlay heatmaps on images/videos showing exactly which pixels influenced the risk score
- **Interactive Evidence Viewer**: Clickable bounding boxes and video timeline navigation
- **Custom Document Detector**: Fine-tune YOLOv8 on specific ID types (passports, credit cards, etc.)

### **Explainability Upgrades**
- **GPT-Powered Narratives**: Generate richer, context-aware explanations using LLMs
- **Cross-Modal Attention Visualization**: Show which text tokens attend to which image regions
- **Real-Time Feedback**: Live analysis during video recording or camera preview

### **Production Optimizations**
- **GPU Acceleration**: Configure CUDA for faster processing
- **Model Fine-Tuning**: Train fusion model on privacy exposure datasets for calibrated scores
- **Multi-Language Support**: Extend to Hindi, Spanish, French, etc.
- **User Accounts**: Save analysis history and track privacy trends over time

### **Advanced Features**
- **Auto-Redaction Tool**: Automatically blur detected PII for safe sharing
- **Browser Extension**: Analyze images before uploading to social media
- **Mobile App**: On-device inference for iOS/Android

---

## üõ°Ô∏è Privacy & Security

- **Local Processing**: All analysis runs locally; no data sent to external servers (except optional GPT API)
- **No Data Storage**: Uploaded files are processed in-memory and deleted immediately
- **Open Source**: Full transparency; audit the code yourself

---

## üìä Performance Tips

### Speed Up Video Processing
- Reduce `max_frames` in `video_analyzer.py` (default: 15)
- Use shorter videos for testing
- Enable GPU acceleration (install `torch` with CUDA)

### Improve Accuracy
- Use higher-resolution images
- Ensure good lighting and focus
- Use clear audio recordings

### GPU Acceleration
```bash
# Install PyTorch with CUDA (if you have an NVIDIA GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

## üîß Troubleshooting

### Common Issues and Solutions

#### **1. FFmpeg Not Found Error**
```
Error: ffmpeg not found
```
**Solution:** Install FFmpeg and add it to your system PATH:
- Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html), extract, and add `bin` folder to PATH
- macOS: `brew install ffmpeg`
- Linux: `sudo apt-get install ffmpeg`

#### **2. spaCy Model Not Found**
```
OSError: [E050] Can't find model 'en_core_web_sm'
```
**Solution:** Download the spaCy model manually:
```bash
python -m spacy download en_core_web_sm
```

#### **3. CUDA Out of Memory (GPU)**
```
RuntimeError: CUDA out of memory
```
**Solution:** 
- Reduce `max_frames` in `backend/app/analysis/video_analyzer.py` (line ~20)
- Process shorter videos
- Use CPU instead of GPU (automatic fallback)

#### **4. Model Download Failures**
```
ConnectionError: Failed to download model
```
**Solution:**
- Check your internet connection
- Models download automatically on first run (may take 10-15 minutes)
- Retry after ensuring stable connection
- Check firewall settings for Hugging Face Hub access

#### **5. CORS Errors in Frontend**
```
Access to fetch at 'http://localhost:8000' has been blocked by CORS policy
```
**Solution:**
- Ensure backend is running on port 8000
- Frontend must run on port 3000
- Check `origins` list in `backend/app/main.py` (lines 69-74)

#### **6. Port Already in Use**
```
ERROR: [Errno 48] Address already in use
```
**Solution:**
- Backend: Change port in uvicorn command: `--port 8001`
- Frontend: Change port in `package.json` or use `PORT=3001 npm run dev`

#### **7. Import Errors After Installation**
```
ModuleNotFoundError: No module named 'easyocr'
```
**Solution:**
- Ensure virtual environment is activated
- Reinstall dependencies: `pip install -r requirements.txt`
- Check Python version (3.8+ required)

#### **8. Video Processing Takes Too Long**
**Solution:**
- Reduce `max_frames` in `video_analyzer.py` (default: 15 ‚Üí try 5-10)
- Use shorter test videos (< 30 seconds)
- Enable GPU acceleration if available

---

## ü§ù Contributing

Contributions are welcome! Areas for improvement:
- Custom document detection dataset
- Better embedding extractors
- Saliency map visualization
- GPT explanation generator
- Frontend overlays
- Performance optimization

---

## üìÑ License

This project is open-source and available under the MIT License.

---

## üôè Acknowledgments

- **YOLOv8** by Ultralytics
- **Whisper** by OpenAI
- **EasyOCR** by JaidedAI
- **spaCy** by Explosion AI
- **Sentence-BERT** by UKPLab
- **Material-UI** by MUI

---

## üìß Contact

For questions, suggestions, or collaboration:
- **GitHub**: [srinivasg0/digital-shadow-analyzer](https://github.com/srinivasg0/digital-shadow-analyzer)

---

## üéØ Why This Project Matters

In an era of oversharing and data breaches, **privacy awareness is critical**. This tool empowers users to:
- **Understand** their digital footprint
- **Detect** hidden PII before it's too late
- **Trust** AI explanations instead of black-box scores

**Digital Shadow Analyzer transforms privacy analysis from opaque to transparent, from reactive to proactive.**

---

## üìã Quick Reference

### **Key Commands**

**Backend:**
```bash
cd backend
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt
python -m spacy download en_core_web_sm
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
npm run dev
```

### **API Endpoints**
- `POST /analyze-text/` - Text PII detection
- `POST /analyze-image/` - Image document detection + OCR
- `POST /analyze-video/` - Video multimodal analysis
- `POST /analyze-audio/` - Audio transcription + PII detection

### **Key Files**
- `backend/app/main.py` - FastAPI endpoints
- `backend/app/analysis/text_analyzer.py` - Text PII detection
- `backend/app/analysis/image_analyzer.py` - YOLOv8 + OCR
- `backend/app/analysis/video_analyzer.py` - Video processing
- `backend/app/fusion/multimodal_fusion.py` - Transformer fusion model
- `backend/requirements.txt` - Python dependencies
- `frontend/src/components/AnalysisTabs.tsx` - Main UI

### **Submission Checklist**
- ‚úÖ Public GitHub repository
- ‚úÖ Comprehensive README.md with setup instructions
- ‚úÖ requirements.txt with all dependencies and versions
- ‚úÖ "Submissions" section with video link
- ‚úÖ Special setup notes for model downloads
- ‚úÖ Troubleshooting guide
- ‚úÖ API documentation
- ‚úÖ Screenshots and examples

---

**Built with ‚ù§Ô∏è for privacy-conscious users and AI enthusiasts.**
